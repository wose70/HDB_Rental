{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wose70/HDB_Rental/blob/main/Python_Script_Rental_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Implementation Guide for Rental Price Prediction System\n",
        "\n",
        "# This guide provides a comprehensive implementation for a rental price prediction system using\n",
        "# the provided Singapore rental market datasets.\n",
        "\n",
        "# 1. Setup and Import Libraries\n",
        "# -----------------------------\n",
        "# Import necessary libraries for data manipulation (pandas, numpy), visualization (matplotlib, seaborn, folium),\n",
        "# machine learning (scikit-learn), deep learning (tensorflow.keras), and geospatial analysis (geopandas).\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import folium # For interactive maps\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV # For splitting data, cross-validation, hyperparameter tuning\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder # For scaling numerical data and encoding categorical data\n",
        "from sklearn.compose import ColumnTransformer # To apply different transformations to different columns\n",
        "from sklearn.pipeline import Pipeline # To chain preprocessing and model steps\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso # Linear models\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor # Ensemble models\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error # Evaluation metrics\n",
        "import xgboost as xgb # XGBoost library (often needs separate installation: pip install xgboost)\n",
        "# Note: The script uses 'XGBRegressor' alias later, let's keep it consistent\n",
        "from xgboost import XGBRegressor\n",
        "from tensorflow.keras.models import Sequential # For building neural networks layer by layer\n",
        "from tensorflow.keras.layers import Dense, Dropout # Core layers for a neural network\n",
        "from tensorflow.keras.optimizers import Adam # Optimizer for training neural networks\n",
        "import geopandas as gpd # For working with geospatial data\n",
        "from shapely.geometry import Point # For creating geometric points from coordinates\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # Suppress warnings (useful for cleaner output, but be cautious in production)\n",
        "\n",
        "# 2. Data Loading\n",
        "# ---------------\n",
        "# Function to load the required datasets. Assumes specific file names.\n",
        "# Make sure these files ('RentingOutofFlats2025.csv', 'street_coordinates.csv',\n",
        "# 'monthly avg and median household income.xlsx') are in the same directory as the script,\n",
        "# or provide the full path.\n",
        "def load_data():\n",
        "    \"\"\"Loads the rental, street coordinates, and income datasets.\"\"\"\n",
        "    try:\n",
        "        # Load the datasets\n",
        "        rental_df = pd.read_csv('RentingOutofFlats2025.csv')\n",
        "        street_coords_df = pd.read_csv('street_coordinates.csv')\n",
        "        # Assuming the income data is in the first sheet of the Excel file\n",
        "        income_df = pd.read_csv('/content/monthly avg and medium household income.csv')\n",
        "\n",
        "        print(f\"Rental data shape: {rental_df.shape}\")\n",
        "        print(f\"Street coordinates data shape: {street_coords_df.shape}\")\n",
        "        print(f\"Income data shape: {income_df.shape}\")\n",
        "\n",
        "        return rental_df, street_coords_df, income_df\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading data: {e}. Make sure the data files are in the correct location.\")\n",
        "        return None, None, None\n",
        "\n",
        "# 3. Data Exploration (EDA)\n",
        "# -------------------------\n",
        "# Function to perform initial Exploratory Data Analysis.\n",
        "# Generates plots to understand data distributions and relationships.\n",
        "def explore_data(rental_df, street_coords_df, income_df):\n",
        "    \"\"\"Performs basic EDA on the rental data and saves plots.\"\"\"\n",
        "    if rental_df is None:\n",
        "        print(\"Rental data not loaded. Skipping exploration.\")\n",
        "        return None\n",
        "\n",
        "    # Basic information about rental data\n",
        "    print(\"\\nRental Data Info:\")\n",
        "    rental_df.info() # Prints column names, non-null counts, and data types\n",
        "    print(\"\\nRental Data Sample:\")\n",
        "    print(rental_df.head()) # Shows the first 5 rows\n",
        "\n",
        "    # Basic statistics for the target variable ('monthly_rent') grouped by 'flat_type'\n",
        "    print(\"\\nRental Price Statistics by Flat Type:\")\n",
        "    print(rental_df.groupby('flat_type')['monthly_rent'].describe())\n",
        "\n",
        "    # Distribution of rental prices (Histogram)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(rental_df['monthly_rent'], kde=True) # kde=True adds a density curve\n",
        "    plt.title('Distribution of Monthly Rent Prices')\n",
        "    plt.xlabel('Monthly Rent (SGD)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "    plt.savefig('rent_distribution.png') # Save the plot as an image\n",
        "    plt.close() # Close the plot to free up memory\n",
        "\n",
        "    # Rental prices by town (Box Plot)\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    # Sort towns by median rent for better visualization\n",
        "    town_order = rental_df.groupby('town')['monthly_rent'].median().sort_values(ascending=False).index\n",
        "    sns.boxplot(x='town', y='monthly_rent', data=rental_df, order=town_order)\n",
        "    plt.title('Rental Prices by Town')\n",
        "    plt.xlabel('Town')\n",
        "    plt.ylabel('Monthly Rent (SGD)')\n",
        "    plt.xticks(rotation=90) # Rotate x-axis labels for readability\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rent_by_town.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Rental prices by flat type (Box Plot)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x='flat_type', y='monthly_rent', data=rental_df)\n",
        "    plt.title('Rental Prices by Flat Type')\n",
        "    plt.xlabel('Flat Type')\n",
        "    plt.ylabel('Monthly Rent (SGD)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rent_by_flat_type.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Check for temporal trends\n",
        "    # Convert 'rent_approval_date' to datetime objects, inferring format if possible\n",
        "    try:\n",
        "        rental_df['rent_approval_date'] = pd.to_datetime(rental_df['rent_approval_date'])\n",
        "        rental_df['year_month'] = rental_df['rent_approval_date'].dt.to_period('M') # Extract Year-Month\n",
        "\n",
        "        # Calculate average rent per month for each flat type\n",
        "        time_trend = rental_df.groupby(['year_month', 'flat_type'])['monthly_rent'].mean().unstack()\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        time_trend.plot(marker='o', ax=plt.gca()) # Plot on the current axes\n",
        "        plt.title('Average Monthly Rent Over Time by Flat Type')\n",
        "        plt.xlabel('Year-Month')\n",
        "        plt.ylabel('Average Monthly Rent (SGD)')\n",
        "        plt.legend(title='Flat Type')\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('rent_time_trend.png')\n",
        "        plt.close()\n",
        "    except KeyError:\n",
        "        print(\"Column 'rent_approval_date' not found. Skipping time trend analysis.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing date for time trend: {e}\")\n",
        "\n",
        "\n",
        "    return rental_df # Return the potentially modified dataframe (with new date columns)\n",
        "\n",
        "# 4. Data Integration\n",
        "# -------------------\n",
        "# Function to merge the different data sources together.\n",
        "def integrate_data(rental_df, street_coords_df, income_df):\n",
        "    \"\"\"Merges rental, coordinates, and income data.\"\"\"\n",
        "    if rental_df is None or street_coords_df is None:\n",
        "        print(\"Required dataframes for integration not loaded. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    # Merge rental data with street coordinates based on 'street_name'\n",
        "    # Using 'left' merge keeps all rows from rental_df\n",
        "    merged_df = pd.merge(\n",
        "        rental_df,\n",
        "        street_coords_df,\n",
        "        on='street_name', # Assumes 'street_name' is the common column\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    print(f\"\\nShape after merging with coordinates: {merged_df.shape}\")\n",
        "    print(f\"Missing values after merging with coordinates:\\n{merged_df.isnull().sum()}\")\n",
        "\n",
        "    # Handle missing coordinates (latitude, longitude) after the merge\n",
        "    # Strategy: Fill missing coordinates for a street with the average for that street.\n",
        "    # If still missing (e.g., street not in coords_df), fill with the average for the town.\n",
        "    # If still missing, fill with the overall average coordinate.\n",
        "    for col in ['latitude', 'longitude']:\n",
        "        if merged_df[col].isnull().sum() > 0:\n",
        "            # Fill with street average (use transform to broadcast the mean back to the original shape)\n",
        "            street_coords_mean = merged_df.groupby('street_name')[col].transform('mean')\n",
        "            merged_df[col].fillna(street_coords_mean, inplace=True)\n",
        "\n",
        "            # Fill remaining NAs with town average\n",
        "            town_coords_mean = merged_df.groupby('town')[col].transform('mean')\n",
        "            merged_df[col].fillna(town_coords_mean, inplace=True)\n",
        "\n",
        "            # Fill any remaining NAs with the overall mean\n",
        "            overall_mean = merged_df[col].mean()\n",
        "            merged_df[col].fillna(overall_mean, inplace=True)\n",
        "            print(f\"Missing values for {col} after imputation: {merged_df[col].isnull().sum()}\")\n",
        "\n",
        "\n",
        "    # Integrate Income Data (Placeholder - Needs Adjustment based on actual income_df structure)\n",
        "    # This section assumes 'income_df' has 'town' and 'median_household_income' columns.\n",
        "    # You MUST inspect 'income_df' and adjust the merging logic accordingly.\n",
        "    if income_df is not None and 'town' in income_df.columns and 'median_household_income' in income_df.columns:\n",
        "        # Example: Aggregate income data by town (e.g., taking the mean if multiple entries per town)\n",
        "        # Adjust the aggregation method (mean, median, first, etc.) as needed.\n",
        "        income_summary = income_df.groupby('town')['median_household_income'].mean().reset_index()\n",
        "\n",
        "        # Merge with the main dataframe\n",
        "        merged_df = pd.merge(\n",
        "            merged_df,\n",
        "            income_summary,\n",
        "            on='town',\n",
        "            how='left' # Keep all rental records, add income where available\n",
        "        )\n",
        "\n",
        "        # Handle missing income data after merge (e.g., fill with overall median/mean)\n",
        "        if 'median_household_income' in merged_df.columns and merged_df['median_household_income'].isnull().sum() > 0:\n",
        "            median_income = merged_df['median_household_income'].median()\n",
        "            merged_df['median_household_income'].fillna(median_income, inplace=True)\n",
        "            print(f\"Missing values for median_household_income after imputation: {merged_df['median_household_income'].isnull().sum()}\")\n",
        "        print(f\"\\nShape after merging with income data: {merged_df.shape}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nIncome data not loaded or required columns ('town', 'median_household_income') not found. Skipping income integration.\")\n",
        "\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "# 5. Feature Engineering\n",
        "# ----------------------\n",
        "# Function to create new features from existing ones.\n",
        "def engineer_features(merged_df):\n",
        "    \"\"\"Creates new features from the merged data.\"\"\"\n",
        "    if merged_df is None:\n",
        "        print(\"Merged dataframe not available. Skipping feature engineering.\")\n",
        "        return None\n",
        "\n",
        "    # --- Date Features ---\n",
        "    # Extract year and month from 'rent_approval_date' if it exists and is datetime\n",
        "    if 'rent_approval_date' in merged_df.columns and pd.api.types.is_datetime64_any_dtype(merged_df['rent_approval_date']):\n",
        "        merged_df['approval_year'] = merged_df['rent_approval_date'].dt.year\n",
        "        merged_df['approval_month'] = merged_df['rent_approval_date'].dt.month\n",
        "        # Optionally drop the original date column if no longer needed for direct modeling\n",
        "        # merged_df = merged_df.drop('rent_approval_date', axis=1)\n",
        "    else:\n",
        "         print(\"Column 'rent_approval_date' not found or not datetime. Skipping date feature extraction.\")\n",
        "\n",
        "\n",
        "    # --- Geospatial Features ---\n",
        "    # Calculate distance to a central point (e.g., CBD)\n",
        "    # Using approximate coordinates for Singapore CBD (Raffles Place)\n",
        "    cbd_lat, cbd_lon = 1.2830, 103.8513 # More precise Raffles Place MRT approx. coords\n",
        "\n",
        "    # Check if latitude and longitude columns exist\n",
        "    if 'latitude' in merged_df.columns and 'longitude' in merged_df.columns:\n",
        "         # Simple Euclidean distance approximation (works reasonably well for small areas like Singapore)\n",
        "         # For higher accuracy over larger distances, use Haversine formula\n",
        "         merged_df['dist_to_cbd_approx'] = np.sqrt(\n",
        "             (merged_df['latitude'] - cbd_lat)**2 +\n",
        "             (merged_df['longitude'] - cbd_lon)**2\n",
        "         )\n",
        "         # Convert approx distance to km (1 degree latitude ~ 111km, 1 degree longitude ~ 111km * cos(latitude))\n",
        "         # Using a simplified factor for Singapore's latitude (~1.35 deg N)\n",
        "         approx_km_factor = 111 * np.cos(np.radians(1.35))\n",
        "         merged_df['dist_to_cbd_km'] = merged_df['dist_to_cbd_approx'] * approx_km_factor\n",
        "\n",
        "    else:\n",
        "        print(\"Latitude or Longitude columns missing. Skipping distance to CBD calculation.\")\n",
        "\n",
        "\n",
        "    # --- Flat Type Features ---\n",
        "    # Extract number of rooms from 'flat_type' (e.g., '3-ROOM' -> 3)\n",
        "    # Handles cases like 'MULTI-GENERATION' or 'EXECUTIVE' where there's no leading digit\n",
        "    if 'flat_type' in merged_df.columns:\n",
        "        merged_df['room_count'] = merged_df['flat_type'].str.extract('^(\\d+)').astype(float)\n",
        "        # Handle non-standard flat types (e.g., EXECUTIVE, MULTI-GENERATION) - fill with a reasonable value or median/mode\n",
        "        # Example: Fill NaN room_count with the median room count\n",
        "        median_rooms = merged_df['room_count'].median()\n",
        "        merged_df['room_count'].fillna(median_rooms, inplace=True)\n",
        "    else:\n",
        "        print(\"Column 'flat_type' not found. Skipping room count extraction.\")\n",
        "\n",
        "\n",
        "    # --- Regional Features ---\n",
        "    # Create broader geographical regions based on towns\n",
        "    # This mapping should be verified or refined based on official planning areas if possible.\n",
        "    if 'town' in merged_df.columns:\n",
        "        region_mapping = {\n",
        "            'ANG MO KIO': 'North-East', 'BEDOK': 'East', 'BISHAN': 'Central', 'BUKIT BATOK': 'West',\n",
        "            'BUKIT MERAH': 'Central', 'BUKIT PANJANG': 'West', 'BUKIT TIMAH': 'Central', # Note: Bukit Timah often considered Central\n",
        "            'CENTRAL AREA': 'Central', 'CHOA CHU KANG': 'West', 'CLEMENTI': 'West', 'GEYLANG': 'Central', # Note: Geylang often considered Central fringe\n",
        "            'HOUGANG': 'North-East', 'JURONG EAST': 'West', 'JURONG WEST': 'West', 'KALLANG/WHAMPOA': 'Central',\n",
        "            'MARINE PARADE': 'East', 'PASIR RIS': 'East', 'PUNGGOL': 'North-East', 'QUEENSTOWN': 'Central',\n",
        "            'SEMBAWANG': 'North', 'SENGKANG': 'North-East', 'SERANGOON': 'North-East', 'TAMPINES': 'East',\n",
        "            'TOA PAYOH': 'Central', 'WOODLANDS': 'North', 'YISHUN': 'North'\n",
        "        }\n",
        "        merged_df['region'] = merged_df['town'].map(region_mapping)\n",
        "        # Handle any towns not in the mapping (assign to 'Other' or a default region)\n",
        "        merged_df['region'].fillna('Other', inplace=True)\n",
        "    else:\n",
        "        print(\"Column 'town' not found. Skipping region mapping.\")\n",
        "\n",
        "    # --- Clean up / Drop unnecessary columns ---\n",
        "    # Drop columns that were used for intermediate steps or are redundant\n",
        "    cols_to_drop = ['rent_approval_date', 'year_month', 'street_name', 'block', 'dist_to_cbd_approx'] # Example list\n",
        "    # Drop only if they exist\n",
        "    merged_df = merged_df.drop(columns=[col for col in cols_to_drop if col in merged_df.columns], axis=1)\n",
        "\n",
        "    print(\"\\nFeatures after engineering:\")\n",
        "    print(merged_df.head())\n",
        "    print(f\"\\nShape after feature engineering: {merged_df.shape}\")\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "\n",
        "# 6. Model Building (Traditional ML)\n",
        "# ----------------------------------\n",
        "# Function to preprocess data, train, and evaluate standard ML models.\n",
        "def build_models(processed_df):\n",
        "    \"\"\"Builds, trains, and evaluates traditional ML regression models.\"\"\"\n",
        "    if processed_df is None:\n",
        "        print(\"Processed dataframe not available. Skipping model building.\")\n",
        "        return {}, None, None, None, None\n",
        "\n",
        "    try:\n",
        "        # Define features (X) and target (y)\n",
        "        X = processed_df.drop('monthly_rent', axis=1)\n",
        "        y = processed_df['monthly_rent']\n",
        "\n",
        "        # Identify categorical and numerical columns *after* feature engineering\n",
        "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "        # Ensure boolean columns are treated as numerical or converted appropriately if needed\n",
        "        numerical_cols = X.select_dtypes(include=np.number).columns.tolist() # Includes int, float, potentially bool\n",
        "\n",
        "        print(f\"\\nNumerical features: {numerical_cols}\")\n",
        "        print(f\"Categorical features: {categorical_cols}\")\n",
        "\n",
        "        # Create a preprocessing pipeline\n",
        "        # Numerical features: Impute missing values (e.g., with median) and scale (e.g., StandardScaler)\n",
        "        # Categorical features: Impute missing values (e.g., with 'missing' category) and one-hot encode\n",
        "        # Note: Imputation might have been done earlier, but adding here makes the pipeline robust\n",
        "        from sklearn.impute import SimpleImputer\n",
        "\n",
        "        numerical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')), # Impute missing numerical values\n",
        "            ('scaler', StandardScaler()) # Scale numerical values\n",
        "        ])\n",
        "\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # Impute missing categorical values\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore')) # One-hot encode, ignore categories not seen in training\n",
        "        ])\n",
        "\n",
        "        # Combine preprocessing steps using ColumnTransformer\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_cols),\n",
        "                ('cat', categorical_transformer, categorical_cols)\n",
        "            ],\n",
        "            remainder='passthrough' # Keep other columns (if any) - should ideally be none if cols are defined correctly\n",
        "            )\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "        print(f\"Testing set shape: {X_test.shape}\")\n",
        "\n",
        "        # Define models to evaluate\n",
        "        models = {\n",
        "            'Linear Regression': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                ('regressor', LinearRegression())\n",
        "            ]),\n",
        "            'Ridge': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                # Ridge adds L2 regularization - alpha controls strength\n",
        "                ('regressor', Ridge(alpha=1.0, random_state=42))\n",
        "            ]),\n",
        "            'Random Forest': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                # Ensemble model - n_estimators = number of trees\n",
        "                # n_jobs=-1 uses all available CPU cores\n",
        "                ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=20, min_samples_split=10)) # Added some common hyperparameters\n",
        "            ]),\n",
        "            'XGBoost': Pipeline([\n",
        "                ('preprocessor', preprocessor),\n",
        "                # Gradient Boosting model - often high performance\n",
        "                ('regressor', XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1, max_depth=7, subsample=0.8)) # Added some common hyperparameters\n",
        "            ])\n",
        "        }\n",
        "\n",
        "        # Train and evaluate each model\n",
        "        results = {}\n",
        "        for name, model in models.items():\n",
        "            print(f\"\\n--- Training {name} ---\")\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Make predictions on the test set\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Evaluate using regression metrics\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            rmse = np.sqrt(mse) # Root Mean Squared Error\n",
        "            mae = mean_absolute_error(y_test, y_pred) # Mean Absolute Error\n",
        "            r2 = r2_score(y_test, y_pred) # R-squared (coefficient of determination)\n",
        "\n",
        "            results[name] = {\n",
        "                'model': model, # Store the trained pipeline\n",
        "                'mse': mse,\n",
        "                'rmse': rmse,\n",
        "                'mae': mae,\n",
        "                'r2': r2\n",
        "            }\n",
        "\n",
        "            print(f\"{name} Results:\")\n",
        "            print(f\"  RMSE: {rmse:.2f}\") # Lower is better\n",
        "            print(f\"  MAE: {mae:.2f}\")  # Lower is better (interpretable in original units)\n",
        "            print(f\"  R²: {r2:.4f}\")   # Closer to 1 is better\n",
        "\n",
        "        # Find best model based on RMSE (you could choose MAE or R2 as well)\n",
        "        best_model_name = min(results, key=lambda k: results[k]['rmse'])\n",
        "        print(f\"\\n== Best Traditional Model (based on RMSE): {best_model_name} with RMSE = {results[best_model_name]['rmse']:.2f} ==\")\n",
        "\n",
        "        return results, X_train, X_test, y_train, y_test, preprocessor # Return preprocessor too\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during model building: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {}, None, None, None, None, None\n",
        "\n",
        "\n",
        "# 7. Neural Network Model (Deep Learning)\n",
        "# ---------------------------------------\n",
        "# Function to build, train, and evaluate a simple feed-forward neural network.\n",
        "def build_neural_network(X_train, y_train, X_test, y_test, preprocessor):\n",
        "    \"\"\"Builds, trains, and evaluates a neural network regression model.\"\"\"\n",
        "    if X_train is None or preprocessor is None:\n",
        "         print(\"Training data or preprocessor not available. Skipping neural network.\")\n",
        "         return None, None\n",
        "\n",
        "    try:\n",
        "        print(\"\\n--- Training Neural Network ---\")\n",
        "        # Apply the *already fitted* preprocessor to the training and test data\n",
        "        # Important: Use transform, not fit_transform on test data!\n",
        "        X_train_processed = preprocessor.transform(X_train)\n",
        "        X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "        # Convert sparse matrix output from OneHotEncoder to dense array if necessary\n",
        "        if hasattr(X_train_processed, \"toarray\"):\n",
        "            X_train_processed = X_train_processed.toarray()\n",
        "        if hasattr(X_test_processed, \"toarray\"):\n",
        "            X_test_processed = X_test_processed.toarray()\n",
        "\n",
        "        # Define the neural network architecture\n",
        "        input_dim = X_train_processed.shape[1] # Number of features after preprocessing\n",
        "\n",
        "        model = Sequential([\n",
        "            Dense(128, activation='relu', input_shape=(input_dim,)), # Input layer + first hidden layer\n",
        "            Dropout(0.2), # Dropout for regularization (prevents overfitting)\n",
        "            Dense(64, activation='relu'), # Second hidden layer\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'), # Third hidden layer\n",
        "            Dense(1) # Output layer (1 neuron for regression, linear activation by default)\n",
        "        ])\n",
        "\n",
        "        # Compile the model\n",
        "        # Adam optimizer is common and effective. Loss 'mse' is standard for regression.\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae']) # mean_absolute_error\n",
        "\n",
        "        model.summary() # Print model architecture\n",
        "\n",
        "        # Set up Early Stopping to prevent overfitting\n",
        "        # Monitors validation loss and stops if it doesn't improve for 'patience' epochs.\n",
        "        # restore_best_weights=True ensures the model weights from the best epoch are kept.\n",
        "        from tensorflow.keras.callbacks import EarlyStopping\n",
        "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
        "\n",
        "        # Train the model\n",
        "        print(\"Training NN...\")\n",
        "        history = model.fit(\n",
        "            X_train_processed, y_train,\n",
        "            epochs=100, # Maximum number of epochs\n",
        "            batch_size=32, # Number of samples per gradient update\n",
        "            validation_split=0.2, # Use 20% of training data for validation during training\n",
        "            callbacks=[early_stop],\n",
        "            verbose=2 # Show one line per epoch\n",
        "        )\n",
        "\n",
        "        # Evaluate the trained model on the test set\n",
        "        loss, mae = model.evaluate(X_test_processed, y_test, verbose=0)\n",
        "        rmse = np.sqrt(loss) # Since loss is MSE\n",
        "        y_pred_nn = model.predict(X_test_processed)\n",
        "        r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "        print(\"\\nNeural Network Evaluation Results:\")\n",
        "        print(f\"  RMSE: {rmse:.2f}\")\n",
        "        print(f\"  MAE: {mae:.2f}\")\n",
        "        print(f\"  R²: {r2_nn:.4f}\")\n",
        "\n",
        "        # Plot training history (Loss and MAE over epochs)\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Plot Loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss During Training')\n",
        "        plt.ylabel('Mean Squared Error (Loss)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot MAE\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['mae'], label='Training MAE')\n",
        "        plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "        plt.title('Model MAE During Training')\n",
        "        plt.ylabel('Mean Absolute Error')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('nn_training_history.png')\n",
        "        plt.close()\n",
        "\n",
        "        nn_metrics = {'mse': loss, 'rmse': rmse, 'mae': mae, 'r2': r2_nn}\n",
        "\n",
        "        # Note: The NN model itself doesn't include the preprocessing steps.\n",
        "        # For prediction, you need both the preprocessor and the NN model.\n",
        "        return model, nn_metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during neural network training: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# 8. Geospatial Analysis and Visualization\n",
        "# ----------------------------------------\n",
        "# Function to create maps and plots showing geographical patterns.\n",
        "def create_geospatial_analysis(processed_df):\n",
        "    \"\"\"Creates geospatial visualizations like heatmaps and regional plots.\"\"\"\n",
        "    if processed_df is None or not all(col in processed_df.columns for col in ['latitude', 'longitude', 'monthly_rent', 'region', 'flat_type']):\n",
        "        print(\"Required columns for geospatial analysis not found. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n--- Creating Geospatial Visualizations ---\")\n",
        "\n",
        "    # --- Heatmap of Rental Prices ---\n",
        "    try:\n",
        "        from folium.plugins import HeatMap\n",
        "\n",
        "        # Create a base map centered on Singapore\n",
        "        singapore_map_heat = folium.Map(location=[1.3521, 103.8198], zoom_start=11) # Slightly zoomed out\n",
        "\n",
        "        # Create heat map data points: list of [latitude, longitude, intensity]\n",
        "        # Filter out rows with missing coordinates\n",
        "        heat_data = processed_df[['latitude', 'longitude', 'monthly_rent']].dropna().values.tolist()\n",
        "\n",
        "        # Add heat map layer to the map\n",
        "        HeatMap(heat_data, radius=10, blur=15).add_to(singapore_map_heat) # Adjust radius and blur as needed\n",
        "\n",
        "        # Save the heatmap to an HTML file\n",
        "        heatmap_filename = 'singapore_rent_heatmap.html'\n",
        "        singapore_map_heat.save(heatmap_filename)\n",
        "        print(f\"Saved rental price heatmap to {heatmap_filename}\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Folium or HeatMap plugin not found. Skipping heatmap generation. Install with: pip install folium\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating heatmap: {e}\")\n",
        "\n",
        "\n",
        "    # --- Bar Plot: Average Rent by Region ---\n",
        "    try:\n",
        "        region_rent = processed_df.groupby('region')['monthly_rent'].mean().reset_index().sort_values('monthly_rent', ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(12, 7))\n",
        "        sns.barplot(x='monthly_rent', y='region', data=region_rent, palette='viridis', orient='h')\n",
        "        plt.title('Average Monthly Rental Prices by Region')\n",
        "        plt.xlabel('Average Monthly Rent (SGD)')\n",
        "        plt.ylabel('Region')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('avg_rent_by_region.png')\n",
        "        plt.close()\n",
        "        print(\"Saved average rent by region plot.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating rent by region plot: {e}\")\n",
        "\n",
        "\n",
        "    # --- Box Plot: Rent Distribution by Region and Flat Type ---\n",
        "    # This can get crowded if there are many regions/types. Consider filtering or alternative plots.\n",
        "    try:\n",
        "        plt.figure(figsize=(16, 10))\n",
        "        # Using town instead of region might be too granular, stick with region\n",
        "        sns.boxplot(x='monthly_rent', y='region', hue='flat_type', data=processed_df, palette='tab10', orient='h')\n",
        "        plt.title('Rental Price Distribution by Region and Flat Type')\n",
        "        plt.xlabel('Monthly Rent (SGD)')\n",
        "        plt.ylabel('Region')\n",
        "        plt.legend(title='Flat Type', bbox_to_anchor=(1.05, 1), loc='upper left') # Place legend outside plot\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('rent_dist_by_region_and_type.png')\n",
        "        plt.close()\n",
        "        print(\"Saved rent distribution by region and flat type plot.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating rent distribution plot: {e}\")\n",
        "\n",
        "    # Optional: Choropleth map requires a GeoJSON file defining region boundaries.\n",
        "    # This is more complex and requires finding or creating a suitable GeoJSON for Singapore regions/towns.\n",
        "\n",
        "# 9. Model Deployment Function (Conceptual)\n",
        "# -----------------------------------------\n",
        "# Function to wrap the prediction logic for new data.\n",
        "def create_prediction_function(best_model, preprocessor):\n",
        "    \"\"\"Creates a function to predict rent for new input data.\"\"\"\n",
        "    if best_model is None or preprocessor is None:\n",
        "        print(\"Model or preprocessor not available. Cannot create prediction function.\")\n",
        "        return None\n",
        "\n",
        "    def predict_rent(input_features_dict):\n",
        "        \"\"\"\n",
        "        Predicts rental price based on input features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        input_features_dict : dict\n",
        "            Dictionary containing feature names and values for a single prediction.\n",
        "            Example: {'town': 'ANG MO KIO', 'flat_type': '4-ROOM', 'latitude': 1.37, 'longitude': 103.85, ...}\n",
        "            Must include all features expected by the preprocessor.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float or None\n",
        "            Predicted monthly rent, or None if prediction fails.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert input dictionary to pandas DataFrame (1 row)\n",
        "            input_df = pd.DataFrame([input_features_dict])\n",
        "\n",
        "            # --- Apply necessary feature engineering steps ---\n",
        "            # This MUST mirror the steps in engineer_features applied to the training data\n",
        "            # Example: Extract room count, map region, calculate distance to CBD\n",
        "            # Ensure consistency! It's often better to wrap feature engineering in a reusable function/class.\n",
        "\n",
        "            # Example: Re-apply some feature engineering (ensure consistency with training)\n",
        "            if 'flat_type' in input_df.columns:\n",
        "                 input_df['room_count'] = input_df['flat_type'].str.extract('^(\\d+)').astype(float).fillna(input_df['room_count'].median()) # Use median from training if needed\n",
        "            if 'town' in input_df.columns:\n",
        "                # Use the same region mapping as in engineer_features\n",
        "                region_mapping = {\n",
        "                    'ANG MO KIO': 'North-East', 'BEDOK': 'East', 'BISHAN': 'Central', 'BUKIT BATOK': 'West',\n",
        "                    'BUKIT MERAH': 'Central', 'BUKIT PANJANG': 'West', 'BUKIT TIMAH': 'Central',\n",
        "                    'CENTRAL AREA': 'Central', 'CHOA CHU KANG': 'West', 'CLEMENTI': 'West', 'GEYLANG': 'Central',\n",
        "                    'HOUGANG': 'North-East', 'JURONG EAST': 'West', 'JURONG WEST': 'West', 'KALLANG/WHAMPOA': 'Central',\n",
        "                    'MARINE PARADE': 'East', 'PASIR RIS': 'East', 'PUNGGOL': 'North-East', 'QUEENSTOWN': 'Central',\n",
        "                    'SEMBAWANG': 'North', 'SENGKANG': 'North-East', 'SERANGOON': 'North-East', 'TAMPINES': 'East',\n",
        "                    'TOA PAYOH': 'Central', 'WOODLANDS': 'North', 'YISHUN': 'North'\n",
        "                 }\n",
        "                input_df['region'] = input_df['town'].map(region_mapping).fillna('Other')\n",
        "            if 'latitude' in input_df.columns and 'longitude' in input_df.columns:\n",
        "                cbd_lat, cbd_lon = 1.2830, 103.8513\n",
        "                approx_km_factor = 111 * np.cos(np.radians(1.35))\n",
        "                input_df['dist_to_cbd_approx'] = np.sqrt(((input_df['latitude'] - cbd_lat)**2) + ((input_df['longitude'] - cbd_lon)**2))\n",
        "                input_df['dist_to_cbd_km'] = input_df['dist_to_cbd_approx'] * approx_km_factor\n",
        "\n",
        "            # Ensure DataFrame has the same columns in the same order as the training data fed to the preprocessor\n",
        "            # You might need to load the column order from training\n",
        "            # X_train_cols = [...] # Load or define column list used for training X\n",
        "            # input_df = input_df.reindex(columns=X_train_cols, fill_value=0) # Or appropriate fill value\n",
        "\n",
        "            # --- Preprocess the input data using the *fitted* preprocessor ---\n",
        "            input_processed = preprocessor.transform(input_df)\n",
        "\n",
        "            # --- Make prediction ---\n",
        "            # Handle potential differences if the best model is NN vs. traditional pipeline\n",
        "            if isinstance(best_model, Pipeline): # If it's a scikit-learn pipeline\n",
        "                 predicted_rent = best_model.predict(input_df)[0] # Pipeline handles preprocessing\n",
        "            elif hasattr(best_model, 'predict') and not isinstance(best_model, Pipeline): # If it's a Keras model (or similar)\n",
        "                 if hasattr(input_processed, \"toarray\"): # Convert sparse to dense if needed for NN\n",
        "                     input_processed = input_processed.toarray()\n",
        "                 predicted_rent = best_model.predict(input_processed)[0][0] # Keras predict might return nested array\n",
        "            else:\n",
        "                 print(\"Error: Unknown model type for prediction.\")\n",
        "                 return None\n",
        "\n",
        "            return float(predicted_rent)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during prediction: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    return predict_rent\n",
        "\n",
        "\n",
        "# 10. Main Execution Block\n",
        "# ------------------------\n",
        "def main():\n",
        "    \"\"\"Main function to run the entire pipeline.\"\"\"\n",
        "    print(\"Starting Rental Price Prediction Pipeline...\")\n",
        "\n",
        "    # 1. Load Data\n",
        "    print(\"\\n[1/7] Loading data...\")\n",
        "    rental_df, street_coords_df, income_df = load_data()\n",
        "    if rental_df is None: return # Stop if data loading failed\n",
        "\n",
        "    # 2. Explore Data\n",
        "    print(\"\\n[2/7] Exploring data...\")\n",
        "    rental_df_explored = explore_data(rental_df.copy(), street_coords_df, income_df) # Use copy to avoid modifying original df in exploration\n",
        "    if rental_df_explored is not None:\n",
        "         rental_df = rental_df_explored # Keep date columns if added\n",
        "\n",
        "    # 3. Integrate Data\n",
        "    print(\"\\n[3/7] Integrating data sources...\")\n",
        "    merged_df = integrate_data(rental_df, street_coords_df, income_df)\n",
        "    if merged_df is None: return\n",
        "\n",
        "    # 4. Engineer Features\n",
        "    print(\"\\n[4/7] Engineering features...\")\n",
        "    processed_df = engineer_features(merged_df.copy()) # Use copy before potential destructive operations\n",
        "    if processed_df is None: return\n",
        "    # Drop rows with NaN in target variable before modeling\n",
        "    processed_df.dropna(subset=['monthly_rent'], inplace=True)\n",
        "\n",
        "\n",
        "    # 5. Build Traditional Models\n",
        "    print(\"\\n[5/7] Building and evaluating traditional ML models...\")\n",
        "    model_results, X_train, X_test, y_train, y_test, preprocessor = build_models(processed_df)\n",
        "    if not model_results: return # Stop if model building failed\n",
        "\n",
        "    # 6. Build Neural Network Model\n",
        "    print(\"\\n[6/7] Building and evaluating neural network model...\")\n",
        "    # Pass the fitted preprocessor from the traditional models step\n",
        "    nn_model, nn_metrics = build_neural_network(X_train, y_train, X_test, y_test, preprocessor)\n",
        "\n",
        "    # --- Determine Overall Best Model ---\n",
        "    best_model = None\n",
        "    best_preprocessor_for_prediction = None # Need the preprocessor associated with the best model\n",
        "\n",
        "    best_traditional_name = min(model_results, key=lambda k: model_results[k]['rmse'])\n",
        "    best_traditional_rmse = model_results[best_traditional_name]['rmse']\n",
        "    print(f\"\\nBest Traditional Model: {best_traditional_name} (RMSE: {best_traditional_rmse:.2f})\")\n",
        "\n",
        "    if nn_model and nn_metrics and nn_metrics['rmse'] < best_traditional_rmse:\n",
        "        print(f\"Neural Network is better (RMSE: {nn_metrics['rmse']:.2f})\")\n",
        "        best_model = nn_model # The Keras model object\n",
        "        # The preprocessor used for NN was the same one fitted in build_models\n",
        "        best_preprocessor_for_prediction = preprocessor\n",
        "        best_model_type = 'Neural Network'\n",
        "    else:\n",
        "        print(f\"{best_traditional_name} is the best overall model.\")\n",
        "        # The best model is the entire scikit-learn pipeline (preprocessor + regressor)\n",
        "        best_model = model_results[best_traditional_name]['model']\n",
        "        # For prediction using the pipeline, we don't need the preprocessor separately\n",
        "        best_preprocessor_for_prediction = preprocessor # Still useful to have for potential separate use\n",
        "        best_model_type = best_traditional_name\n",
        "\n",
        "    # 7. Geospatial Analysis (using the final processed data)\n",
        "    print(\"\\n[7/7] Creating geospatial visualizations...\")\n",
        "    create_geospatial_analysis(processed_df)\n",
        "\n",
        "    # --- Create Prediction Function ---\n",
        "    print(\"\\n--- Setting up Prediction Function ---\")\n",
        "    predict_rent_func = create_prediction_function(best_model, best_preprocessor_for_prediction)\n",
        "\n",
        "    # --- Example Prediction ---\n",
        "    if predict_rent_func:\n",
        "        print(\"\\n--- Example Prediction ---\")\n",
        "        # Define sample input - MUST match features used in training!\n",
        "        # Get column names from the preprocessor's fitted transformers\n",
        "        try:\n",
        "             # Get feature names after one-hot encoding etc.\n",
        "             # This is complex to get perfectly right, especially post-pipeline.\n",
        "             # Easier: Use the columns from the original X_train dataframe\n",
        "             example_input_features = X_train.iloc[0].to_dict() # Take the first row of training data as example\n",
        "             print(f\"Using example features: {example_input_features}\")\n",
        "\n",
        "             # Modify a few values for the example prediction\n",
        "             example_input_features['town'] = 'TAMPINES'\n",
        "             example_input_features['flat_type'] = '5-ROOM'\n",
        "             # You might need to add/adjust other features like latitude, longitude, room_count etc.\n",
        "             # based on the 'TAMPINES'/'5-ROOM' choice, or use averages.\n",
        "             # For simplicity, we'll use the rest from the original example row.\n",
        "\n",
        "             predicted_price = predict_rent_func(example_input_features)\n",
        "\n",
        "             if predicted_price is not None:\n",
        "                 print(f\"\\nPredicted rent for example flat ({example_input_features['flat_type']} in {example_input_features['town']}): SGD ${predicted_price:.2f}\")\n",
        "             else:\n",
        "                 print(\"\\nSample prediction failed.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Error during example prediction: {e}\")\n",
        "\n",
        "\n",
        "    print(\"\\nPipeline execution finished.\")\n",
        "\n",
        "# Ensures that the main() function is called only when the script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Rental Price Prediction Pipeline...\n",
            "\n",
            "[1/7] Loading data...\n",
            "Rental data shape: (155464, 6)\n",
            "Street coordinates data shape: (589, 3)\n",
            "Income data shape: (25, 3)\n",
            "\n",
            "[2/7] Exploring data...\n",
            "\n",
            "Rental Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 155464 entries, 0 to 155463\n",
            "Data columns (total 6 columns):\n",
            " #   Column              Non-Null Count   Dtype \n",
            "---  ------              --------------   ----- \n",
            " 0   rent_approval_date  155464 non-null  object\n",
            " 1   town                155464 non-null  object\n",
            " 2   block               155464 non-null  object\n",
            " 3   street_name         155464 non-null  object\n",
            " 4   flat_type           155464 non-null  object\n",
            " 5   monthly_rent        155464 non-null  int64 \n",
            "dtypes: int64(1), object(5)\n",
            "memory usage: 7.1+ MB\n",
            "\n",
            "Rental Data Sample:\n",
            "  rent_approval_date        town block       street_name flat_type  \\\n",
            "0            2021-01  ANG MO KIO   105  ANG MO KIO AVE 4    4-ROOM   \n",
            "1            2021-01  ANG MO KIO   107  ANG MO KIO AVE 4    3-ROOM   \n",
            "2            2021-01  ANG MO KIO   108  ANG MO KIO AVE 4    3-ROOM   \n",
            "3            2021-01  ANG MO KIO   111  ANG MO KIO AVE 4    5-ROOM   \n",
            "4            2021-01  ANG MO KIO   111  ANG MO KIO AVE 4    5-ROOM   \n",
            "\n",
            "   monthly_rent  \n",
            "0          2000  \n",
            "1          1750  \n",
            "2          1750  \n",
            "3          2230  \n",
            "4          2450  \n",
            "\n",
            "Rental Price Statistics by Flat Type:\n",
            "             count         mean         std    min     25%     50%     75%  \\\n",
            "flat_type                                                                    \n",
            "1-ROOM        48.0  1525.520833  439.883011  800.0  1175.0  1550.0  2000.0   \n",
            "2-ROOM      3234.0  1978.433519  575.788370  300.0  1550.0  2100.0  2450.0   \n",
            "3-ROOM     50751.0  2387.590313  593.557524  450.0  1900.0  2350.0  2850.0   \n",
            "4-ROOM     56053.0  2841.030150  728.738619  580.0  2220.0  2800.0  3400.0   \n",
            "5-ROOM     36787.0  2984.347487  765.228258  600.0  2350.0  3000.0  3500.0   \n",
            "EXECUTIVE   8591.0  3074.361890  798.932530  750.0  2400.0  3000.0  3700.0   \n",
            "\n",
            "              max  \n",
            "flat_type          \n",
            "1-ROOM     2150.0  \n",
            "2-ROOM     4000.0  \n",
            "3-ROOM     6300.0  \n",
            "4-ROOM     6200.0  \n",
            "5-ROOM     7600.0  \n",
            "EXECUTIVE  6600.0  \n",
            "\n",
            "[3/7] Integrating data sources...\n",
            "\n",
            "Shape after merging with coordinates: (155464, 9)\n",
            "Missing values after merging with coordinates:\n",
            "rent_approval_date      0\n",
            "town                    0\n",
            "block                   0\n",
            "street_name             0\n",
            "flat_type               0\n",
            "monthly_rent            0\n",
            "year_month              0\n",
            "latitude              153\n",
            "longitude             153\n",
            "dtype: int64\n",
            "Missing values for latitude after imputation: 0\n",
            "Missing values for longitude after imputation: 0\n",
            "\n",
            "Income data not loaded or required columns ('town', 'median_household_income') not found. Skipping income integration.\n",
            "\n",
            "[4/7] Engineering features...\n",
            "\n",
            "Features after engineering:\n",
            "         town flat_type  monthly_rent  latitude   longitude  approval_year  \\\n",
            "0  ANG MO KIO    4-ROOM          2000  1.383825  103.837538           2021   \n",
            "1  ANG MO KIO    3-ROOM          1750  1.383825  103.837538           2021   \n",
            "2  ANG MO KIO    3-ROOM          1750  1.383825  103.837538           2021   \n",
            "3  ANG MO KIO    5-ROOM          2230  1.383825  103.837538           2021   \n",
            "4  ANG MO KIO    5-ROOM          2450  1.383825  103.837538           2021   \n",
            "\n",
            "   approval_month  dist_to_cbd_km  room_count      region  \n",
            "0               1        11.29223         4.0  North-East  \n",
            "1               1        11.29223         3.0  North-East  \n",
            "2               1        11.29223         3.0  North-East  \n",
            "3               1        11.29223         5.0  North-East  \n",
            "4               1        11.29223         5.0  North-East  \n",
            "\n",
            "Shape after feature engineering: (155464, 10)\n",
            "\n",
            "[5/7] Building and evaluating traditional ML models...\n",
            "\n",
            "Numerical features: ['latitude', 'longitude', 'approval_year', 'approval_month', 'dist_to_cbd_km', 'room_count']\n",
            "Categorical features: ['town', 'flat_type', 'region']\n",
            "\n",
            "Training set shape: (124371, 9)\n",
            "Testing set shape: (31093, 9)\n",
            "\n",
            "--- Training Linear Regression ---\n",
            "Linear Regression Results:\n",
            "  RMSE: 535.19\n",
            "  MAE: 408.72\n",
            "  R²: 0.4993\n",
            "\n",
            "--- Training Ridge ---\n",
            "Ridge Results:\n",
            "  RMSE: 535.20\n",
            "  MAE: 408.73\n",
            "  R²: 0.4993\n",
            "\n",
            "--- Training Random Forest ---\n",
            "Random Forest Results:\n",
            "  RMSE: 508.47\n",
            "  MAE: 383.11\n",
            "  R²: 0.5481\n",
            "\n",
            "--- Training XGBoost ---\n",
            "XGBoost Results:\n",
            "  RMSE: 491.41\n",
            "  MAE: 371.78\n",
            "  R²: 0.5779\n",
            "\n",
            "== Best Traditional Model (based on RMSE): XGBoost with RMSE = 491.41 ==\n",
            "\n",
            "[6/7] Building and evaluating neural network model...\n",
            "\n",
            "--- Training Neural Network ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m5,888\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,888</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,257\u001b[0m (63.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,257</span> (63.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,257\u001b[0m (63.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,257</span> (63.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training NN...\n",
            "Epoch 1/100\n",
            "3110/3110 - 12s - 4ms/step - loss: 616512.3750 - mae: 539.0908 - val_loss: 277242.2500 - val_mae: 402.5110\n",
            "Epoch 2/100\n",
            "3110/3110 - 19s - 6ms/step - loss: 332474.2812 - mae: 444.1286 - val_loss: 272885.7500 - val_mae: 402.0751\n",
            "Epoch 3/100\n",
            "3110/3110 - 9s - 3ms/step - loss: 330184.0625 - mae: 442.3075 - val_loss: 283592.3750 - val_mae: 405.5075\n",
            "Epoch 4/100\n",
            "3110/3110 - 11s - 3ms/step - loss: 329384.2812 - mae: 441.8941 - val_loss: 275962.0000 - val_mae: 400.6779\n",
            "Epoch 5/100\n",
            "3110/3110 - 19s - 6ms/step - loss: 329046.1562 - mae: 441.7492 - val_loss: 273045.0938 - val_mae: 399.5038\n",
            "Epoch 6/100\n",
            "3110/3110 - 9s - 3ms/step - loss: 327143.1250 - mae: 439.8705 - val_loss: 273502.7500 - val_mae: 399.4178\n",
            "Epoch 7/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 327237.3438 - mae: 439.7630 - val_loss: 270122.5000 - val_mae: 398.6215\n",
            "Epoch 8/100\n",
            "3110/3110 - 11s - 4ms/step - loss: 326629.0000 - mae: 439.4319 - val_loss: 271777.6875 - val_mae: 397.1398\n",
            "Epoch 9/100\n",
            "3110/3110 - 20s - 7ms/step - loss: 325446.3125 - mae: 438.4319 - val_loss: 271565.6875 - val_mae: 397.1014\n",
            "Epoch 10/100\n",
            "3110/3110 - 11s - 3ms/step - loss: 327637.9375 - mae: 440.3238 - val_loss: 274117.7188 - val_mae: 399.7278\n",
            "Epoch 11/100\n",
            "3110/3110 - 19s - 6ms/step - loss: 324414.6875 - mae: 437.5072 - val_loss: 273583.9375 - val_mae: 399.0647\n",
            "Epoch 12/100\n",
            "3110/3110 - 11s - 4ms/step - loss: 324393.9688 - mae: 437.9768 - val_loss: 272874.0312 - val_mae: 398.5043\n",
            "Epoch 13/100\n",
            "3110/3110 - 19s - 6ms/step - loss: 323403.3125 - mae: 438.0972 - val_loss: 269987.5312 - val_mae: 396.9764\n",
            "Epoch 14/100\n",
            "3110/3110 - 13s - 4ms/step - loss: 323508.1250 - mae: 437.6108 - val_loss: 273822.9688 - val_mae: 401.9374\n",
            "Epoch 15/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 322504.7812 - mae: 437.0184 - val_loss: 280593.4375 - val_mae: 407.7133\n",
            "Epoch 16/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 323041.1562 - mae: 436.2421 - val_loss: 270842.3125 - val_mae: 398.8468\n",
            "Epoch 17/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 322321.7500 - mae: 437.1159 - val_loss: 270565.0625 - val_mae: 396.2402\n",
            "Epoch 18/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 321364.2188 - mae: 435.5426 - val_loss: 288327.5000 - val_mae: 409.1582\n",
            "Epoch 19/100\n",
            "3110/3110 - 20s - 6ms/step - loss: 320240.9375 - mae: 434.6898 - val_loss: 270504.6562 - val_mae: 399.2145\n",
            "Epoch 20/100\n",
            "3110/3110 - 12s - 4ms/step - loss: 319788.7812 - mae: 434.7916 - val_loss: 269726.6562 - val_mae: 397.8839\n",
            "Epoch 21/100\n",
            "3110/3110 - 20s - 6ms/step - loss: 319824.8438 - mae: 434.0385 - val_loss: 272230.8750 - val_mae: 397.6899\n",
            "Epoch 22/100\n",
            "3110/3110 - 21s - 7ms/step - loss: 317430.7500 - mae: 432.6722 - val_loss: 274810.3438 - val_mae: 400.6405\n",
            "Epoch 23/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 316858.3438 - mae: 432.3644 - val_loss: 273050.6562 - val_mae: 398.4966\n",
            "Epoch 24/100\n",
            "3110/3110 - 11s - 4ms/step - loss: 315372.7812 - mae: 431.5741 - val_loss: 269982.3750 - val_mae: 395.9585\n",
            "Epoch 25/100\n",
            "3110/3110 - 19s - 6ms/step - loss: 316149.5938 - mae: 431.1942 - val_loss: 269477.5312 - val_mae: 396.8079\n",
            "Epoch 26/100\n",
            "3110/3110 - 11s - 3ms/step - loss: 313845.0625 - mae: 430.1891 - val_loss: 270133.0312 - val_mae: 397.3549\n",
            "Epoch 27/100\n",
            "3110/3110 - 20s - 6ms/step - loss: 312756.8750 - mae: 429.0816 - val_loss: 274257.3438 - val_mae: 399.1767\n",
            "Epoch 28/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 310073.1562 - mae: 426.6391 - val_loss: 274283.9375 - val_mae: 399.8975\n",
            "Epoch 29/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 310813.2812 - mae: 427.8879 - val_loss: 274216.4688 - val_mae: 402.2283\n",
            "Epoch 30/100\n",
            "3110/3110 - 9s - 3ms/step - loss: 307942.4375 - mae: 426.1478 - val_loss: 263933.5625 - val_mae: 392.5996\n",
            "Epoch 31/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 301944.9375 - mae: 422.5670 - val_loss: 257368.3906 - val_mae: 388.9295\n",
            "Epoch 32/100\n",
            "3110/3110 - 13s - 4ms/step - loss: 298404.5625 - mae: 420.0930 - val_loss: 251713.2656 - val_mae: 382.6472\n",
            "Epoch 33/100\n",
            "3110/3110 - 11s - 4ms/step - loss: 293151.3125 - mae: 415.8806 - val_loss: 251535.4062 - val_mae: 379.7212\n",
            "Epoch 34/100\n",
            "3110/3110 - 20s - 6ms/step - loss: 291337.9062 - mae: 413.7223 - val_loss: 257762.4375 - val_mae: 383.8554\n",
            "Epoch 35/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 290283.1875 - mae: 412.4225 - val_loss: 254697.8594 - val_mae: 386.9735\n",
            "Epoch 36/100\n",
            "3110/3110 - 11s - 3ms/step - loss: 290087.1562 - mae: 412.0170 - val_loss: 249706.8750 - val_mae: 378.7288\n",
            "Epoch 37/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 289456.9375 - mae: 411.6905 - val_loss: 252770.3594 - val_mae: 384.7709\n",
            "Epoch 38/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 288679.7500 - mae: 410.7916 - val_loss: 251268.3594 - val_mae: 378.8140\n",
            "Epoch 39/100\n",
            "3110/3110 - 11s - 4ms/step - loss: 286178.9688 - mae: 409.6487 - val_loss: 251343.7344 - val_mae: 380.5452\n",
            "Epoch 40/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 285884.9375 - mae: 408.6328 - val_loss: 260532.3125 - val_mae: 392.5507\n",
            "Epoch 41/100\n",
            "3110/3110 - 21s - 7ms/step - loss: 285852.3125 - mae: 408.8903 - val_loss: 252218.6719 - val_mae: 383.9224\n",
            "Epoch 42/100\n",
            "3110/3110 - 21s - 7ms/step - loss: 285687.7812 - mae: 408.4785 - val_loss: 249456.8906 - val_mae: 378.9238\n",
            "Epoch 43/100\n",
            "3110/3110 - 12s - 4ms/step - loss: 282686.4375 - mae: 406.4572 - val_loss: 258007.8438 - val_mae: 388.2271\n",
            "Epoch 44/100\n",
            "3110/3110 - 12s - 4ms/step - loss: 281405.4062 - mae: 405.2539 - val_loss: 253348.6094 - val_mae: 385.6952\n",
            "Epoch 45/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 281221.6562 - mae: 404.7212 - val_loss: 250945.5312 - val_mae: 382.3134\n",
            "Epoch 46/100\n",
            "3110/3110 - 11s - 3ms/step - loss: 279582.1250 - mae: 404.0174 - val_loss: 248739.7031 - val_mae: 378.3162\n",
            "Epoch 47/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 278617.4062 - mae: 403.1093 - val_loss: 254582.6406 - val_mae: 379.4239\n",
            "Epoch 48/100\n",
            "3110/3110 - 21s - 7ms/step - loss: 277609.4688 - mae: 401.9584 - val_loss: 251250.0625 - val_mae: 380.7256\n",
            "Epoch 49/100\n",
            "3110/3110 - 11s - 4ms/step - loss: 276013.5000 - mae: 401.2260 - val_loss: 249222.2656 - val_mae: 380.0482\n",
            "Epoch 50/100\n",
            "3110/3110 - 20s - 6ms/step - loss: 276486.0000 - mae: 401.3732 - val_loss: 249374.0312 - val_mae: 380.6275\n",
            "Epoch 51/100\n",
            "3110/3110 - 21s - 7ms/step - loss: 275060.2500 - mae: 400.1685 - val_loss: 248557.9375 - val_mae: 380.3777\n",
            "Epoch 52/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 273440.0000 - mae: 398.4156 - val_loss: 249088.1406 - val_mae: 379.8767\n",
            "Epoch 53/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 272208.0625 - mae: 397.5675 - val_loss: 255860.0938 - val_mae: 389.6034\n",
            "Epoch 54/100\n",
            "3110/3110 - 11s - 3ms/step - loss: 271812.5625 - mae: 396.7433 - val_loss: 248997.2656 - val_mae: 381.8525\n",
            "Epoch 55/100\n",
            "3110/3110 - 19s - 6ms/step - loss: 269492.6562 - mae: 395.7629 - val_loss: 254918.4844 - val_mae: 387.2880\n",
            "Epoch 56/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 270502.8125 - mae: 395.7314 - val_loss: 251090.8438 - val_mae: 385.6826\n",
            "Epoch 57/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 267907.0938 - mae: 394.1984 - val_loss: 250275.6875 - val_mae: 378.7388\n",
            "Epoch 58/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 267801.1250 - mae: 394.2312 - val_loss: 250488.7344 - val_mae: 379.2882\n",
            "Epoch 59/100\n",
            "3110/3110 - 9s - 3ms/step - loss: 266193.5312 - mae: 392.2332 - val_loss: 248097.8750 - val_mae: 379.2572\n",
            "Epoch 60/100\n",
            "3110/3110 - 11s - 3ms/step - loss: 266231.1875 - mae: 392.1596 - val_loss: 252572.2188 - val_mae: 385.1704\n",
            "Epoch 61/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 264942.2500 - mae: 391.5233 - val_loss: 250870.3906 - val_mae: 383.9743\n",
            "Epoch 62/100\n",
            "3110/3110 - 11s - 3ms/step - loss: 264020.2812 - mae: 390.7412 - val_loss: 248347.4844 - val_mae: 379.2777\n",
            "Epoch 63/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 263858.1875 - mae: 390.7123 - val_loss: 249227.5156 - val_mae: 382.7713\n",
            "Epoch 64/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 262611.0312 - mae: 389.8240 - val_loss: 249210.0938 - val_mae: 381.8384\n",
            "Epoch 65/100\n",
            "3110/3110 - 11s - 3ms/step - loss: 261675.7031 - mae: 388.8424 - val_loss: 248396.6562 - val_mae: 379.8519\n",
            "Epoch 66/100\n",
            "3110/3110 - 21s - 7ms/step - loss: 261074.7344 - mae: 388.2654 - val_loss: 249239.4531 - val_mae: 383.2838\n",
            "Epoch 67/100\n",
            "3110/3110 - 20s - 6ms/step - loss: 260768.3750 - mae: 387.8238 - val_loss: 250136.4844 - val_mae: 383.6430\n",
            "Epoch 68/100\n",
            "3110/3110 - 9s - 3ms/step - loss: 259699.7812 - mae: 387.4701 - val_loss: 249485.2969 - val_mae: 380.9620\n",
            "Epoch 69/100\n",
            "3110/3110 - 10s - 3ms/step - loss: 259187.7188 - mae: 386.8227 - val_loss: 248429.7031 - val_mae: 377.7093\n",
            "Epoch 69: early stopping\n",
            "Restoring model weights from the end of the best epoch: 59.\n",
            "\u001b[1m972/972\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "\n",
            "Neural Network Evaluation Results:\n",
            "  RMSE: 505.20\n",
            "  MAE: 384.85\n",
            "  R²: 0.5539\n",
            "\n",
            "Best Traditional Model: XGBoost (RMSE: 491.41)\n",
            "XGBoost is the best overall model.\n",
            "\n",
            "[7/7] Creating geospatial visualizations...\n",
            "\n",
            "--- Creating Geospatial Visualizations ---\n",
            "Saved rental price heatmap to singapore_rent_heatmap.html\n",
            "Saved average rent by region plot.\n",
            "Saved rent distribution by region and flat type plot.\n",
            "\n",
            "--- Setting up Prediction Function ---\n",
            "\n",
            "--- Example Prediction ---\n",
            "Using example features: {'town': 'SENGKANG', 'flat_type': '4-ROOM', 'latitude': 1.388201321, 'longitude': 103.9038973, 'approval_year': 2023, 'approval_month': 1, 'dist_to_cbd_km': 13.051879805239011, 'room_count': 4.0, 'region': 'North-East'}\n",
            "\n",
            "Predicted rent for example flat (5-ROOM in TAMPINES): SGD $3116.44\n",
            "\n",
            "Pipeline execution finished.\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FmCASIdeapiF",
        "outputId": "5bd5c0aa-127b-453b-a1a3-3b6f09717a24"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}